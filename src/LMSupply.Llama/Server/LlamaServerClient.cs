using System.Net.Http.Json;
using System.Runtime.CompilerServices;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;

namespace LMSupply.Llama.Server;

/// <summary>
/// OpenAI-compatible HTTP client for llama-server.
/// </summary>
public sealed class LlamaServerClient : IDisposable
{
    private readonly HttpClient _httpClient;
    private readonly string _baseUrl;
    private readonly bool _ownsHttpClient;

    private static readonly JsonSerializerOptions JsonOptions = new()
    {
        PropertyNamingPolicy = JsonNamingPolicy.SnakeCaseLower,
        DefaultIgnoreCondition = JsonIgnoreCondition.WhenWritingNull
    };

    /// <summary>
    /// Creates a new client for the specified server URL.
    /// </summary>
    public LlamaServerClient(string baseUrl, HttpClient? httpClient = null)
    {
        _baseUrl = baseUrl.TrimEnd('/');

        if (httpClient != null)
        {
            _httpClient = httpClient;
            _ownsHttpClient = false;
        }
        else
        {
            _httpClient = new HttpClient();
            _ownsHttpClient = true;
        }
    }

    /// <summary>
    /// Generates a streaming chat completion.
    /// </summary>
    public async IAsyncEnumerable<string> GenerateChatAsync(
        IEnumerable<ChatCompletionMessage> messages,
        ChatCompletionOptions? options = null,
        [EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        options ??= new ChatCompletionOptions();

        var request = new ChatCompletionRequest
        {
            Messages = messages.ToList(),
            MaxTokens = options.MaxTokens,
            Temperature = options.Temperature,
            TopP = options.TopP,
            TopK = options.TopK > 0 ? options.TopK : null,
            MinP = options.MinP > 0 ? options.MinP : null,
            RepeatPenalty = options.RepeatPenalty != 1.0f ? options.RepeatPenalty : null,
            FrequencyPenalty = options.FrequencyPenalty != 0 ? options.FrequencyPenalty : null,
            PresencePenalty = options.PresencePenalty != 0 ? options.PresencePenalty : null,
            Seed = options.Seed != -1 ? options.Seed : null,
            Stream = true,
            Stop = options.StopSequences?.ToList(),
            Grammar = options.Grammar,
            JsonSchema = options.JsonSchema
        };

        var json = JsonSerializer.Serialize(request, JsonOptions);
        using var content = new StringContent(json, Encoding.UTF8, "application/json");

        using var httpRequest = new HttpRequestMessage(HttpMethod.Post, $"{_baseUrl}/v1/chat/completions")
        {
            Content = content
        };

        using var response = await _httpClient.SendAsync(
            httpRequest,
            HttpCompletionOption.ResponseHeadersRead,
            cancellationToken);

        response.EnsureSuccessStatusCode();

        await using var stream = await response.Content.ReadAsStreamAsync(cancellationToken);
        using var reader = new StreamReader(stream);

        while (!reader.EndOfStream)
        {
            var line = await reader.ReadLineAsync(cancellationToken);

            if (string.IsNullOrEmpty(line))
                continue;

            if (!line.StartsWith("data: "))
                continue;

            var data = line[6..];

            if (data == "[DONE]")
                break;

            ChatCompletionChunk? chunk;
            try
            {
                chunk = JsonSerializer.Deserialize<ChatCompletionChunk>(data, JsonOptions);
            }
            catch
            {
                continue;
            }

            var delta = chunk?.Choices?.FirstOrDefault()?.Delta?.Content;
            if (!string.IsNullOrEmpty(delta))
            {
                yield return delta;
            }
        }
    }

    /// <summary>
    /// Generates a non-streaming chat completion.
    /// </summary>
    public async Task<string> GenerateChatCompleteAsync(
        IEnumerable<ChatCompletionMessage> messages,
        ChatCompletionOptions? options = null,
        CancellationToken cancellationToken = default)
    {
        var sb = new StringBuilder();

        await foreach (var token in GenerateChatAsync(messages, options, cancellationToken))
        {
            sb.Append(token);
        }

        return sb.ToString();
    }

    /// <summary>
    /// Generates a streaming text completion.
    /// </summary>
    public async IAsyncEnumerable<string> GenerateAsync(
        string prompt,
        CompletionOptions? options = null,
        [EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        options ??= new CompletionOptions();

        var request = new CompletionRequest
        {
            Prompt = prompt,
            NPredict = options.MaxTokens,
            Temperature = options.Temperature,
            TopP = options.TopP,
            TopK = options.TopK > 0 ? options.TopK : null,
            MinP = options.MinP > 0 ? options.MinP : null,
            RepeatPenalty = options.RepeatPenalty != 1.0f ? options.RepeatPenalty : null,
            FrequencyPenalty = options.FrequencyPenalty != 0 ? options.FrequencyPenalty : null,
            PresencePenalty = options.PresencePenalty != 0 ? options.PresencePenalty : null,
            Seed = options.Seed != -1 ? options.Seed : null,
            Stream = true,
            Stop = options.StopSequences?.ToList(),
            Grammar = options.Grammar,
            JsonSchema = options.JsonSchema
        };

        var json = JsonSerializer.Serialize(request, JsonOptions);
        using var content = new StringContent(json, Encoding.UTF8, "application/json");

        using var httpRequest = new HttpRequestMessage(HttpMethod.Post, $"{_baseUrl}/completion")
        {
            Content = content
        };

        using var response = await _httpClient.SendAsync(
            httpRequest,
            HttpCompletionOption.ResponseHeadersRead,
            cancellationToken);

        response.EnsureSuccessStatusCode();

        await using var stream = await response.Content.ReadAsStreamAsync(cancellationToken);
        using var reader = new StreamReader(stream);

        while (!reader.EndOfStream)
        {
            var line = await reader.ReadLineAsync(cancellationToken);

            if (string.IsNullOrEmpty(line))
                continue;

            if (!line.StartsWith("data: "))
                continue;

            var data = line[6..];

            CompletionChunk? chunk;
            try
            {
                chunk = JsonSerializer.Deserialize<CompletionChunk>(data, JsonOptions);
            }
            catch
            {
                continue;
            }

            if (chunk?.Stop == true)
                break;

            if (!string.IsNullOrEmpty(chunk?.Content))
            {
                yield return chunk.Content;
            }
        }
    }

    /// <summary>
    /// Checks if the server is healthy.
    /// </summary>
    public async Task<bool> CheckHealthAsync(CancellationToken cancellationToken = default)
    {
        try
        {
            var response = await _httpClient.GetAsync($"{_baseUrl}/health", cancellationToken);
            return response.IsSuccessStatusCode;
        }
        catch
        {
            return false;
        }
    }

    #region Embedding API

    /// <summary>
    /// Generates embeddings for a single text input.
    /// Requires server started with --embedding flag.
    /// </summary>
    public async Task<float[]> GenerateEmbeddingAsync(
        string input,
        CancellationToken cancellationToken = default)
    {
        var result = await GenerateEmbeddingsBatchAsync([input], cancellationToken);
        return result[0];
    }

    /// <summary>
    /// Generates embeddings for multiple text inputs in batch.
    /// Requires server started with --embedding flag.
    /// </summary>
    public async Task<float[][]> GenerateEmbeddingsBatchAsync(
        IReadOnlyList<string> inputs,
        CancellationToken cancellationToken = default)
    {
        var request = new EmbeddingRequest
        {
            Input = inputs.Count == 1 ? inputs[0] : inputs
        };

        var json = JsonSerializer.Serialize(request, JsonOptions);
        using var content = new StringContent(json, Encoding.UTF8, "application/json");

        using var response = await _httpClient.PostAsync(
            $"{_baseUrl}/v1/embeddings",
            content,
            cancellationToken);

        response.EnsureSuccessStatusCode();

        var responseJson = await response.Content.ReadAsStringAsync(cancellationToken);
        var embeddingResponse = JsonSerializer.Deserialize<EmbeddingResponse>(responseJson, JsonOptions);

        if (embeddingResponse?.Data == null || embeddingResponse.Data.Count == 0)
        {
            throw new InvalidOperationException("No embeddings returned from server");
        }

        // Sort by index to ensure correct order
        return embeddingResponse.Data
            .OrderBy(d => d.Index)
            .Select(d => d.Embedding)
            .ToArray();
    }

    #endregion

    #region Reranking API

    /// <summary>
    /// Reranks documents by relevance to a query.
    /// Requires server started with --embedding and --pooling rank flags.
    /// </summary>
    public async Task<IReadOnlyList<RerankResult>> RerankAsync(
        string query,
        IReadOnlyList<string> documents,
        int topN = 10,
        CancellationToken cancellationToken = default)
    {
        var request = new RerankRequest
        {
            Query = query,
            Documents = documents.ToList(),
            TopN = Math.Min(topN, documents.Count)
        };

        var json = JsonSerializer.Serialize(request, JsonOptions);
        using var content = new StringContent(json, Encoding.UTF8, "application/json");

        using var response = await _httpClient.PostAsync(
            $"{_baseUrl}/v1/rerank",
            content,
            cancellationToken);

        response.EnsureSuccessStatusCode();

        var responseJson = await response.Content.ReadAsStringAsync(cancellationToken);
        var rerankResponse = JsonSerializer.Deserialize<RerankResponse>(responseJson, JsonOptions);

        return rerankResponse?.Results ?? [];
    }

    #endregion

    public void Dispose()
    {
        if (_ownsHttpClient)
        {
            _httpClient.Dispose();
        }
    }
}

#region Request/Response Models

/// <summary>
/// Chat completion message.
/// </summary>
public sealed class ChatCompletionMessage
{
    [JsonPropertyName("role")]
    public required string Role { get; init; }

    [JsonPropertyName("content")]
    public required string Content { get; init; }

    public static ChatCompletionMessage System(string content) => new() { Role = "system", Content = content };
    public static ChatCompletionMessage User(string content) => new() { Role = "user", Content = content };
    public static ChatCompletionMessage Assistant(string content) => new() { Role = "assistant", Content = content };
}

/// <summary>
/// Options for chat completion.
/// </summary>
public sealed class ChatCompletionOptions
{
    public int MaxTokens { get; init; } = 256;
    public float Temperature { get; init; } = 0.7f;
    public float TopP { get; init; } = 0.9f;
    public int TopK { get; init; } = 50;
    public float MinP { get; init; } = 0.05f;
    public float RepeatPenalty { get; init; } = 1.1f;
    public float FrequencyPenalty { get; init; } = 0.0f;
    public float PresencePenalty { get; init; } = 0.0f;
    public int Seed { get; init; } = -1;
    public IReadOnlyList<string>? StopSequences { get; init; }

    /// <summary>
    /// Grammar constraint in GBNF format (Phase 3).
    /// </summary>
    public string? Grammar { get; init; }

    /// <summary>
    /// JSON schema for structured output (Phase 3).
    /// When set, output will be constrained to match this schema.
    /// </summary>
    public string? JsonSchema { get; init; }
}

/// <summary>
/// Options for text completion.
/// </summary>
public sealed class CompletionOptions
{
    public int MaxTokens { get; init; } = 256;
    public float Temperature { get; init; } = 0.7f;
    public float TopP { get; init; } = 0.9f;
    public int TopK { get; init; } = 50;
    public float MinP { get; init; } = 0.05f;
    public float RepeatPenalty { get; init; } = 1.1f;
    public float FrequencyPenalty { get; init; } = 0.0f;
    public float PresencePenalty { get; init; } = 0.0f;
    public int Seed { get; init; } = -1;
    public IReadOnlyList<string>? StopSequences { get; init; }

    /// <summary>
    /// Grammar constraint in GBNF format (Phase 3).
    /// </summary>
    public string? Grammar { get; init; }

    /// <summary>
    /// JSON schema for structured output (Phase 3).
    /// When set, output will be constrained to match this schema.
    /// </summary>
    public string? JsonSchema { get; init; }
}

internal sealed class ChatCompletionRequest
{
    public List<ChatCompletionMessage>? Messages { get; set; }
    public int? MaxTokens { get; set; }
    public float? Temperature { get; set; }
    public float? TopP { get; set; }
    public int? TopK { get; set; }
    public float? MinP { get; set; }
    public float? RepeatPenalty { get; set; }
    public float? FrequencyPenalty { get; set; }
    public float? PresencePenalty { get; set; }
    public int? Seed { get; set; }
    public bool Stream { get; set; }
    public List<string>? Stop { get; set; }

    /// <summary>
    /// Grammar constraint in GBNF format (Phase 3).
    /// </summary>
    public string? Grammar { get; set; }

    /// <summary>
    /// JSON schema for structured output (Phase 3).
    /// </summary>
    public string? JsonSchema { get; set; }

    /// <summary>
    /// Re-use KV cache from previous request if possible.
    /// Reduces first token latency for prompts with common prefixes.
    /// </summary>
    public bool CachePrompt { get; set; } = true;
}

internal sealed class CompletionRequest
{
    public string? Prompt { get; set; }
    public int? NPredict { get; set; }
    public float? Temperature { get; set; }
    public float? TopP { get; set; }
    public int? TopK { get; set; }
    public float? MinP { get; set; }
    public float? RepeatPenalty { get; set; }
    public float? FrequencyPenalty { get; set; }
    public float? PresencePenalty { get; set; }
    public int? Seed { get; set; }
    public bool Stream { get; set; }
    public List<string>? Stop { get; set; }

    /// <summary>
    /// Grammar constraint in GBNF format (Phase 3).
    /// </summary>
    public string? Grammar { get; set; }

    /// <summary>
    /// JSON schema for structured output (Phase 3).
    /// </summary>
    public string? JsonSchema { get; set; }

    /// <summary>
    /// Re-use KV cache from previous request if possible.
    /// Reduces first token latency for prompts with common prefixes.
    /// </summary>
    public bool CachePrompt { get; set; } = true;
}

internal sealed class ChatCompletionChunk
{
    public List<ChatCompletionChoice>? Choices { get; set; }
}

internal sealed class ChatCompletionChoice
{
    public ChatCompletionDelta? Delta { get; set; }
    public string? FinishReason { get; set; }
}

internal sealed class ChatCompletionDelta
{
    public string? Content { get; set; }
}

internal sealed class CompletionChunk
{
    public string? Content { get; set; }
    public bool Stop { get; set; }
}

#endregion

#region Embedding Request/Response Models

internal sealed class EmbeddingRequest
{
    /// <summary>
    /// Input text(s) to embed. Can be a single string or array of strings.
    /// </summary>
    public required object Input { get; set; }

    /// <summary>
    /// Model identifier (optional, defaults to loaded model).
    /// </summary>
    public string Model { get; set; } = "default";

    /// <summary>
    /// Encoding format: "float" (default) or "base64".
    /// </summary>
    public string EncodingFormat { get; set; } = "float";
}

internal sealed class EmbeddingResponse
{
    public string Object { get; set; } = "list";
    public List<EmbeddingData> Data { get; set; } = [];
    public string Model { get; set; } = "";
    public EmbeddingUsage Usage { get; set; } = new();
}

internal sealed class EmbeddingData
{
    public string Object { get; set; } = "embedding";
    public float[] Embedding { get; set; } = [];
    public int Index { get; set; }
}

internal sealed class EmbeddingUsage
{
    public int PromptTokens { get; set; }
    public int TotalTokens { get; set; }
}

#endregion

#region Reranking Request/Response Models

internal sealed class RerankRequest
{
    /// <summary>
    /// The search query.
    /// </summary>
    public required string Query { get; set; }

    /// <summary>
    /// Documents to rerank.
    /// </summary>
    public required List<string> Documents { get; set; }

    /// <summary>
    /// Maximum number of results to return.
    /// </summary>
    public int TopN { get; set; } = 10;
}

internal sealed class RerankResponse
{
    public List<RerankResult> Results { get; set; } = [];
}

/// <summary>
/// Result from reranking operation.
/// </summary>
public sealed class RerankResult
{
    /// <summary>
    /// Original index of the document in the input list.
    /// </summary>
    public int Index { get; set; }

    /// <summary>
    /// Relevance score (higher is more relevant).
    /// </summary>
    public float RelevanceScore { get; set; }
}

#endregion
