using LMSupply.Hardware;
using LMSupply.Runtime;

namespace LMSupply.Generator;

/// <summary>
/// Quantization types for KV cache memory optimization.
/// </summary>
public enum KvCacheQuantizationType
{
    /// <summary>
    /// 16-bit floating point (default, highest quality).
    /// </summary>
    F16 = 0,

    /// <summary>
    /// 8-bit quantization. Good balance of memory savings and quality.
    /// Reduces KV cache memory by ~50% with minimal quality loss.
    /// </summary>
    Q8_0 = 1,

    /// <summary>
    /// 4-bit quantization. Maximum memory savings.
    /// Reduces KV cache memory by ~75% but may affect output quality.
    /// </summary>
    Q4_0 = 2,

    /// <summary>
    /// 32-bit floating point. Maximum quality, highest memory usage.
    /// </summary>
    F32 = 3
}

/// <summary>
/// Advanced configuration options for GGUF model loading and inference via llama-server.
/// These options provide fine-grained control over llama.cpp backend behavior.
/// </summary>
public sealed class LlamaOptions
{
    /// <summary>
    /// Gets or sets the number of layers to offload to GPU.
    /// -1 = All layers on GPU (default for GPU providers)
    /// 0 = CPU only
    /// N = Offload N layers to GPU
    /// </summary>
    /// <remarks>
    /// If <see cref="GpuOffloadRatio"/> is set, it takes precedence over this value.
    /// </remarks>
    public int? GpuLayerCount { get; set; }

    /// <summary>
    /// Gets or sets the GPU offload ratio (0.0 to 1.0).
    /// 0.0 = CPU only (equivalent to GpuLayerCount = 0)
    /// 1.0 = All layers on GPU (equivalent to GpuLayerCount = -1)
    /// 0.5 = 50% of layers on GPU
    /// </summary>
    /// <remarks>
    /// This provides a more intuitive way to control GPU usage.
    /// When set, this value takes precedence over <see cref="GpuLayerCount"/>.
    /// </remarks>
    public float? GpuOffloadRatio { get; set; }

    /// <summary>
    /// Gets or sets the batch size for prompt processing.
    /// Higher values improve throughput but use more memory.
    /// Defaults to 512.
    /// </summary>
    public uint? BatchSize { get; set; }

    /// <summary>
    /// Gets or sets the RoPE frequency base for context extension.
    /// Use with RoPE-scaling-aware models for extended context lengths.
    /// </summary>
    public float? RopeFrequencyBase { get; set; }

    /// <summary>
    /// Gets or sets the RoPE frequency scale factor.
    /// Use with RoPE-scaling-aware models for extended context lengths.
    /// </summary>
    public float? RopeFrequencyScale { get; set; }

    /// <summary>
    /// Gets or sets whether to use Flash Attention for improved performance.
    /// Requires compatible GPU (CUDA compute capability 7.0+).
    /// Defaults to false for compatibility.
    /// </summary>
    public bool? FlashAttention { get; set; }

    /// <summary>
    /// Gets or sets whether to use memory mapping for model loading.
    /// Enables faster model loading and reduced memory usage when the same model
    /// is loaded multiple times. Defaults to true.
    /// </summary>
    public bool? UseMemoryMap { get; set; }

    /// <summary>
    /// Gets or sets whether to lock model memory to prevent swapping.
    /// Improves inference latency but may require elevated privileges.
    /// Defaults to false.
    /// </summary>
    public bool? UseMemoryLock { get; set; }

    /// <summary>
    /// Gets or sets the main GPU index for multi-GPU systems.
    /// 0-based index of the GPU to use as the primary device.
    /// </summary>
    public int? MainGpu { get; set; }

    /// <summary>
    /// Gets or sets the number of threads for CPU computation.
    /// Defaults to system-detected optimal thread count.
    /// </summary>
    public int? Threads { get; set; }

    /// <summary>
    /// Gets or sets the quantization type for KV cache keys.
    /// Q8_0 offers good memory/quality balance, Q4_0 for maximum memory savings.
    /// Defaults to F16 (no quantization).
    /// </summary>
    public KvCacheQuantizationType? TypeK { get; set; }

    /// <summary>
    /// Gets or sets the quantization type for KV cache values.
    /// Q8_0 offers good memory/quality balance, Q4_0 for maximum memory savings.
    /// Defaults to F16 (no quantization).
    /// </summary>
    public KvCacheQuantizationType? TypeV { get; set; }

    /// <summary>
    /// Gets or sets the physical batch size for prompt processing.
    /// Smaller than BatchSize, controls memory usage during processing.
    /// Defaults to 512.
    /// </summary>
    public uint? UBatchSize { get; set; }

    #region Phase 3: Multimodal Support

    /// <summary>
    /// Gets or sets the path to a multimodal projector file (mmproj).
    /// Required for vision models like LLaVA, Qwen-VL, InternVL, etc.
    /// The projector bridges visual features with the language model.
    /// </summary>
    public string? MultimodalProjector { get; set; }

    #endregion

    #region Phase 3: LoRA Support

    /// <summary>
    /// Gets or sets the path to a LoRA adapter file.
    /// Allows loading fine-tuned adapters without modifying base model weights.
    /// </summary>
    public string? LoraPath { get; set; }

    /// <summary>
    /// Gets or sets the LoRA adapter scale factor.
    /// Controls the strength of LoRA adaptation. Default: 1.0.
    /// Lower values reduce the adapter's effect, higher values increase it.
    /// </summary>
    public float? LoraScale { get; set; }

    #endregion

    /// <summary>
    /// Gets optimal LlamaOptions based on current hardware profile.
    /// </summary>
    /// <returns>LlamaOptions configured for optimal performance on detected hardware.</returns>
    public static LlamaOptions GetOptimalForHardware()
    {
        var profile = HardwareProfile.Current;

        // FlashAttention support depends on GPU vendor and tier
        // - NVIDIA CUDA: Compute capability 7.0+ (Volta and newer)
        // - Apple Metal: Supported on Apple Silicon
        // - Vulkan/Hip: Not reliably supported, disable by default
        var flashAttention = ShouldEnableFlashAttention(profile);

        return profile.Tier switch
        {
            PerformanceTier.Ultra => new LlamaOptions
            {
                GpuLayerCount = -1,          // All layers on GPU
                BatchSize = 4096,            // Large batch for fast prompt processing (TTFT)
                UBatchSize = 1024,           // Physical batch size
                FlashAttention = flashAttention,
                UseMemoryMap = true,
                UseMemoryLock = false,       // Don't require elevated privileges
                TypeK = KvCacheQuantizationType.Q8_0,  // Quantized KV cache for memory savings
                TypeV = KvCacheQuantizationType.Q8_0
            },
            PerformanceTier.High => new LlamaOptions
            {
                GpuLayerCount = -1,
                BatchSize = 2048,            // Larger batch for faster first token
                UBatchSize = 512,
                FlashAttention = flashAttention,
                UseMemoryMap = true,
                UseMemoryLock = false,
                TypeK = KvCacheQuantizationType.Q8_0,
                TypeV = KvCacheQuantizationType.Q8_0
            },
            PerformanceTier.Medium => new LlamaOptions
            {
                GpuLayerCount = -1,
                BatchSize = 1024,            // Increased for better TTFT
                UBatchSize = 512,
                FlashAttention = false,      // Conservative for mid-range
                UseMemoryMap = true,
                UseMemoryLock = false,
                TypeK = KvCacheQuantizationType.Q4_0,  // More aggressive quantization for memory
                TypeV = KvCacheQuantizationType.Q4_0
            },
            _ => new LlamaOptions              // Low tier
            {
                GpuLayerCount = 0,            // CPU only for safety
                BatchSize = 512,             // Reasonable batch for prompt processing
                UBatchSize = 256,
                FlashAttention = false,
                UseMemoryMap = true,
                UseMemoryLock = false
                // No KV cache quantization for CPU (default F16)
            }
        };
    }

    /// <summary>
    /// Determines if FlashAttention should be enabled based on hardware.
    /// </summary>
    private static bool ShouldEnableFlashAttention(HardwareProfile profile)
    {
        // Only enable for high-performance tiers
        if (profile.Tier < PerformanceTier.High)
            return false;

        var gpuInfo = profile.GpuInfo;

        return gpuInfo.Vendor switch
        {
            // NVIDIA: Enable for compute capability 7.0+ (Volta, Turing, Ampere, Ada, Hopper)
            GpuVendor.Nvidia when gpuInfo.CudaComputeCapabilityMajor >= 7 => true,

            // Apple Silicon: Metal supports FlashAttention
            GpuVendor.Apple => true,

            // AMD/Intel/Others: Vulkan and Hip backends don't reliably support FlashAttention
            // Wait for llama.cpp to improve Vulkan FlashAttention support
            _ => false
        };
    }

    /// <summary>
    /// Calculates the effective GPU layer count, considering both GpuLayerCount and GpuOffloadRatio.
    /// </summary>
    /// <param name="totalLayers">Total number of layers in the model.</param>
    /// <returns>The number of layers to offload to GPU.</returns>
    public int GetEffectiveGpuLayerCount(int totalLayers)
    {
        // GpuOffloadRatio takes precedence if set
        if (GpuOffloadRatio.HasValue)
        {
            var ratio = Math.Clamp(GpuOffloadRatio.Value, 0f, 1f);

            return ratio switch
            {
                0f => 0,                           // CPU only
                >= 1f => -1,                       // All layers on GPU
                _ => (int)(totalLayers * ratio)   // Partial offload
            };
        }

        // Fall back to GpuLayerCount
        return GpuLayerCount ?? -1; // Default to all layers on GPU
    }

    /// <summary>
    /// Creates options with the specified GPU offload ratio.
    /// </summary>
    /// <param name="ratio">Offload ratio: 0.0 (CPU) to 1.0 (full GPU).</param>
    /// <returns>A new LlamaOptions instance.</returns>
    public static LlamaOptions WithGpuRatio(float ratio)
    {
        return new LlamaOptions
        {
            GpuOffloadRatio = Math.Clamp(ratio, 0f, 1f)
        };
    }

    /// <summary>
    /// Creates CPU-only options.
    /// </summary>
    public static LlamaOptions CpuOnly => new() { GpuOffloadRatio = 0f };

    /// <summary>
    /// Creates full GPU offload options.
    /// </summary>
    public static LlamaOptions FullGpu => new() { GpuOffloadRatio = 1f };
}
