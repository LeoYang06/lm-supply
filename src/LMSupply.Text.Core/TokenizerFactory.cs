namespace LMSupply.Text;

/// <summary>
/// Factory for creating tokenizer instances from model directories.
/// </summary>
public static class TokenizerFactory
{
    /// <summary>
    /// Creates a WordPiece tokenizer (BERT-style) from model directory.
    /// </summary>
    /// <param name="modelDir">Path to model directory.</param>
    /// <param name="maxSequenceLength">Maximum sequence length.</param>
    /// <returns>A sequence tokenizer instance.</returns>
    public static async Task<ISequenceTokenizer> CreateWordPieceAsync(
        string modelDir,
        int maxSequenceLength = 512)
    {
        var vocabPath = Path.Combine(modelDir, "vocab.txt");
        var tokenizerJsonPath = Path.Combine(modelDir, "tokenizer.json");

        Tokenizer tokenizer;
        SpecialTokens specialTokens;

        if (File.Exists(vocabPath))
        {
            // Load from vocab.txt
            using var vocabStream = File.OpenRead(vocabPath);
            tokenizer = WordPieceTokenizer.Create(vocabStream);
            var vocab = await VocabularyLoader.LoadFromVocabTxtAsync(vocabPath);
            specialTokens = SpecialTokens.FromVocabulary(vocab);
        }
        else if (File.Exists(tokenizerJsonPath))
        {
            // Extract vocab from tokenizer.json and create WordPiece tokenizer
            tokenizer = CreateWordPieceFromJson(tokenizerJsonPath);
            specialTokens = VocabularyLoader.ExtractSpecialTokensFromJson(tokenizerJsonPath);
        }
        else
        {
            throw new FileNotFoundException(
                $"No vocabulary file found. Expected vocab.txt or tokenizer.json in: {modelDir}");
        }

        return new WordPieceSequenceTokenizer(tokenizer, specialTokens, maxSequenceLength);
    }

    /// <summary>
    /// Creates a WordPiece pair tokenizer (for cross-encoders/rerankers).
    /// </summary>
    /// <param name="modelDir">Path to model directory.</param>
    /// <param name="maxSequenceLength">Maximum sequence length.</param>
    /// <returns>A pair tokenizer instance.</returns>
    public static async Task<IPairTokenizer> CreateWordPiecePairAsync(
        string modelDir,
        int maxSequenceLength = 512)
    {
        var vocabPath = Path.Combine(modelDir, "vocab.txt");
        var tokenizerJsonPath = Path.Combine(modelDir, "tokenizer.json");

        Tokenizer tokenizer;
        SpecialTokens specialTokens;

        if (File.Exists(vocabPath))
        {
            using var vocabStream = File.OpenRead(vocabPath);
            tokenizer = WordPieceTokenizer.Create(vocabStream);
            var vocab = await VocabularyLoader.LoadFromVocabTxtAsync(vocabPath);
            specialTokens = SpecialTokens.FromVocabulary(vocab);
        }
        else if (File.Exists(tokenizerJsonPath))
        {
            tokenizer = CreateWordPieceFromJson(tokenizerJsonPath);
            specialTokens = VocabularyLoader.ExtractSpecialTokensFromJson(tokenizerJsonPath);
        }
        else
        {
            throw new FileNotFoundException(
                $"No vocabulary file found. Expected vocab.txt or tokenizer.json in: {modelDir}");
        }

        return new WordPiecePairTokenizer(tokenizer, specialTokens, maxSequenceLength);
    }

    /// <summary>
    /// Creates a SentencePiece tokenizer (for translation models).
    /// </summary>
    /// <param name="modelDir">Path to model directory.</param>
    /// <returns>A text tokenizer instance.</returns>
    public static ITextTokenizer CreateSentencePiece(string modelDir)
    {
        var spmPath = FindSentencePieceModel(modelDir);
        var vocab = LoadVocabularySync(modelDir);
        var specialTokens = SpecialTokens.FromVocabulary(vocab);

        Tokenizer tokenizer;
        if (spmPath != null)
        {
            using var stream = File.OpenRead(spmPath);
            tokenizer = LlamaTokenizer.Create(stream);
        }
        else
        {
            // Fallback to BPE if SentencePiece not found
            tokenizer = CreateBpeTokenizer(modelDir)
                ?? throw new FileNotFoundException(
                    $"No SentencePiece model found. Expected .spm or .model file in: {modelDir}");
        }

        return new SentencePieceTextTokenizer(tokenizer, specialTokens);
    }

    /// <summary>
    /// Creates a GPT-2 style BPE tokenizer.
    /// </summary>
    /// <param name="modelDir">Path to model directory.</param>
    /// <returns>A text tokenizer instance.</returns>
    public static ITextTokenizer CreateGpt2(string modelDir)
    {
        var vocabPath = Path.Combine(modelDir, "vocab.json");
        var mergesPath = Path.Combine(modelDir, "merges.txt");

        if (!File.Exists(vocabPath) || !File.Exists(mergesPath))
        {
            throw new FileNotFoundException(
                $"GPT-2 tokenizer requires vocab.json and merges.txt in: {modelDir}");
        }

        using var vocabStream = File.OpenRead(vocabPath);
        using var mergesStream = File.OpenRead(mergesPath);
        var tokenizer = CodeGenTokenizer.Create(vocabStream, mergesStream);

        return new Gpt2TextTokenizer(tokenizer);
    }

    /// <summary>
    /// Auto-detects and creates appropriate tokenizer from model directory.
    /// </summary>
    /// <param name="modelDir">Path to model directory.</param>
    /// <param name="maxSequenceLength">Maximum sequence length (for sequence tokenizers).</param>
    /// <returns>A tokenizer instance.</returns>
    public static async Task<ITextTokenizer> CreateAutoAsync(
        string modelDir,
        int maxSequenceLength = 512)
    {
        // Check for SentencePiece model
        if (FindSentencePieceModel(modelDir) != null)
        {
            return CreateSentencePiece(modelDir);
        }

        // Check for GPT-2 style (vocab.json + merges.txt)
        var mergesPath = Path.Combine(modelDir, "merges.txt");
        var vocabJsonPath = Path.Combine(modelDir, "vocab.json");
        if (File.Exists(mergesPath) && File.Exists(vocabJsonPath))
        {
            return CreateGpt2(modelDir);
        }

        // Check for BERT style (vocab.txt or tokenizer.json with WordPiece)
        var vocabTxtPath = Path.Combine(modelDir, "vocab.txt");
        var tokenizerJsonPath = Path.Combine(modelDir, "tokenizer.json");
        if (File.Exists(vocabTxtPath) || File.Exists(tokenizerJsonPath))
        {
            return await CreateWordPieceAsync(modelDir, maxSequenceLength);
        }

        throw new FileNotFoundException(
            $"Could not determine tokenizer type from: {modelDir}. " +
            "Expected vocab.txt, vocab.json + merges.txt, tokenizer.json, or .spm model");
    }

    /// <summary>
    /// Creates a SentencePiece/Unigram pair tokenizer (for cross-encoders/rerankers with non-WordPiece models).
    /// </summary>
    /// <param name="modelDir">Path to model directory.</param>
    /// <param name="maxSequenceLength">Maximum sequence length.</param>
    /// <returns>A pair tokenizer instance.</returns>
    public static async Task<IPairTokenizer> CreateSentencePiecePairAsync(
        string modelDir,
        int maxSequenceLength = 512)
    {
        var tokenizerJsonPath = Path.Combine(modelDir, "tokenizer.json");
        var spmPath = FindSentencePieceModel(modelDir);

        Tokenizer tokenizer;
        SpecialTokens specialTokens;

        if (spmPath != null)
        {
            using var stream = File.OpenRead(spmPath);
            tokenizer = LlamaTokenizer.Create(stream);
            var vocab = LoadVocabularySync(modelDir);
            specialTokens = SpecialTokens.FromVocabulary(vocab);
        }
        else if (File.Exists(tokenizerJsonPath))
        {
            // For Unigram models without .spm file, try to create from tokenizer.json
            tokenizer = await CreateTokenizerFromJsonAsync(tokenizerJsonPath);
            specialTokens = VocabularyLoader.ExtractSpecialTokensFromJson(tokenizerJsonPath);
        }
        else
        {
            // Fallback to BPE
            var bpeTokenizer = CreateBpeTokenizer(modelDir);
            if (bpeTokenizer == null)
            {
                throw new FileNotFoundException(
                    $"No SentencePiece/BPE model found in: {modelDir}");
            }
            tokenizer = bpeTokenizer;
            var vocab = LoadVocabularySync(modelDir);
            specialTokens = SpecialTokens.FromVocabulary(vocab);
        }

        return new SentencePiecePairTokenizer(tokenizer, specialTokens, maxSequenceLength);
    }

    /// <summary>
    /// Auto-detects tokenizer type and creates appropriate pair tokenizer from model directory.
    /// Supports WordPiece, Unigram, and BPE tokenizers.
    /// </summary>
    /// <param name="modelDir">Path to model directory.</param>
    /// <param name="maxSequenceLength">Maximum sequence length.</param>
    /// <returns>A pair tokenizer instance.</returns>
    public static async Task<IPairTokenizer> CreateAutoPairAsync(
        string modelDir,
        int maxSequenceLength = 512)
    {
        var tokenizerJsonPath = Path.Combine(modelDir, "tokenizer.json");
        var vocabTxtPath = Path.Combine(modelDir, "vocab.txt");

        // If vocab.txt exists, use WordPiece (BERT-style)
        if (File.Exists(vocabTxtPath))
        {
            return await CreateWordPiecePairAsync(modelDir, maxSequenceLength);
        }

        // Check tokenizer.json for model type
        if (File.Exists(tokenizerJsonPath))
        {
            var tokenizerType = DetectTokenizerType(tokenizerJsonPath);

            return tokenizerType switch
            {
                "WordPiece" => await CreateWordPiecePairAsync(modelDir, maxSequenceLength),
                "Unigram" or "BPE" => await CreateSentencePiecePairAsync(modelDir, maxSequenceLength),
                _ => await CreateSentencePiecePairAsync(modelDir, maxSequenceLength)
            };
        }

        // Check for SentencePiece model
        if (FindSentencePieceModel(modelDir) != null)
        {
            return await CreateSentencePiecePairAsync(modelDir, maxSequenceLength);
        }

        throw new FileNotFoundException(
            $"Could not determine tokenizer type from: {modelDir}. " +
            "Expected vocab.txt, tokenizer.json, or .spm model");
    }

    /// <summary>
    /// Detects the tokenizer type from tokenizer.json.
    /// </summary>
    private static string? DetectTokenizerType(string tokenizerJsonPath)
    {
        try
        {
            var json = File.ReadAllText(tokenizerJsonPath);
            using var doc = JsonDocument.Parse(json);

            if (doc.RootElement.TryGetProperty("model", out var model) &&
                model.TryGetProperty("type", out var typeElement))
            {
                return typeElement.GetString();
            }
        }
        catch
        {
            // Fall through
        }

        return null;
    }

    /// <summary>
    /// Creates a tokenizer from tokenizer.json for non-WordPiece models.
    /// </summary>
    private static async Task<Tokenizer> CreateTokenizerFromJsonAsync(string tokenizerJsonPath)
    {
        var json = await File.ReadAllTextAsync(tokenizerJsonPath);
        using var doc = JsonDocument.Parse(json);

        if (!doc.RootElement.TryGetProperty("model", out var model))
        {
            throw new InvalidOperationException("Invalid tokenizer.json: missing 'model' section");
        }

        if (!model.TryGetProperty("vocab", out var vocab))
        {
            throw new InvalidOperationException("Invalid tokenizer.json: missing 'model.vocab' section");
        }

        // Build vocab dictionary sorted by ID
        var vocabDict = new SortedDictionary<int, string>();

        // Handle both Object and Array formats for vocab
        if (vocab.ValueKind == JsonValueKind.Object)
        {
            foreach (var property in vocab.EnumerateObject())
            {
                vocabDict[property.Value.GetInt32()] = property.Name;
            }
        }
        else if (vocab.ValueKind == JsonValueKind.Array)
        {
            // Handle array formats:
            // 1. [{"id": 0, "content": "[PAD]"}, ...] - Object items
            // 2. [["token", score], ...] - Unigram format (tuple-like arrays)
            var index = 0;
            foreach (var item in vocab.EnumerateArray())
            {
                if (item.ValueKind == JsonValueKind.Object)
                {
                    // Object format: {"id": 0, "content": "[PAD]"}
                    if (item.TryGetProperty("id", out var idProp) &&
                        item.TryGetProperty("content", out var contentProp))
                    {
                        vocabDict[idProp.GetInt32()] = contentProp.GetString() ?? string.Empty;
                    }
                }
                else if (item.ValueKind == JsonValueKind.Array)
                {
                    // Unigram format: ["token", score] - index is the token ID
                    var arr = item.EnumerateArray().ToArray();
                    if (arr.Length >= 1 && arr[0].ValueKind == JsonValueKind.String)
                    {
                        vocabDict[index] = arr[0].GetString() ?? string.Empty;
                    }
                }
                index++;
            }
        }
        else
        {
            throw new InvalidOperationException(
                $"Invalid tokenizer.json: 'model.vocab' has unexpected type '{vocab.ValueKind}'.");
        }

        if (vocabDict.Count == 0)
        {
            throw new InvalidOperationException("Invalid tokenizer.json: 'model.vocab' is empty");
        }

        // Try to detect tokenizer type and handle appropriately
        var tokenizerType = model.TryGetProperty("type", out var typeElement)
            ? typeElement.GetString()
            : null;

        // For Unigram models, we need to create a different tokenizer
        // Try using the BPE approach with vocab
        var vocabJsonPath = Path.Combine(Path.GetDirectoryName(tokenizerJsonPath)!, "vocab.json");
        var mergesPath = Path.Combine(Path.GetDirectoryName(tokenizerJsonPath)!, "merges.txt");

        if (File.Exists(vocabJsonPath) && File.Exists(mergesPath))
        {
            using var vocabStream = File.OpenRead(vocabJsonPath);
            using var mergesStream = File.OpenRead(mergesPath);
            return CodeGenTokenizer.Create(vocabStream, mergesStream);
        }

        // Fallback: Create a simple vocab-based tokenizer using WordPiece format
        // This works for many Unigram models that have BERT-compatible special tokens
        var vocabLines = new StringBuilder();
        for (var i = 0; i < vocabDict.Count; i++)
        {
            vocabLines.AppendLine(vocabDict.TryGetValue(i, out var token) ? token : $"[unused{i}]");
        }

        var vocabBytes = Encoding.UTF8.GetBytes(vocabLines.ToString());
        using var vocabStream2 = new MemoryStream(vocabBytes);
        return WordPieceTokenizer.Create(vocabStream2);
    }

    private static Tokenizer CreateWordPieceFromJson(string tokenizerJsonPath)
    {
        var json = File.ReadAllText(tokenizerJsonPath);
        using var doc = JsonDocument.Parse(json);

        if (!doc.RootElement.TryGetProperty("model", out var model))
        {
            throw new InvalidOperationException("Invalid tokenizer.json: missing 'model' section");
        }

        // Check model type - WordPiece tokenizer only works with WordPiece models
        if (model.TryGetProperty("type", out var modelType))
        {
            var typeStr = modelType.GetString();
            if (typeStr != null && !typeStr.Equals("WordPiece", StringComparison.OrdinalIgnoreCase))
            {
                throw new InvalidOperationException(
                    $"Tokenizer type mismatch: expected 'WordPiece' but found '{typeStr}'. " +
                    $"This model may require a different tokenizer (e.g., SentencePiece for BPE/Unigram models).");
            }
        }

        if (!model.TryGetProperty("vocab", out var vocab))
        {
            throw new InvalidOperationException("Invalid tokenizer.json: missing 'model.vocab' section");
        }

        // Build vocab dictionary sorted by ID
        var vocabDict = new SortedDictionary<int, string>();

        // Handle both Object and Array formats for vocab
        if (vocab.ValueKind == JsonValueKind.Object)
        {
            foreach (var property in vocab.EnumerateObject())
            {
                vocabDict[property.Value.GetInt32()] = property.Name;
            }
        }
        else if (vocab.ValueKind == JsonValueKind.Array)
        {
            // Handle array formats:
            // 1. [{"id": 0, "content": "[PAD]"}, ...] - Object items
            // 2. [["token", score], ...] - Unigram format (tuple-like arrays)
            var index = 0;
            foreach (var item in vocab.EnumerateArray())
            {
                if (item.ValueKind == JsonValueKind.Object)
                {
                    // Object format: {"id": 0, "content": "[PAD]"}
                    if (item.TryGetProperty("id", out var idProp) &&
                        item.TryGetProperty("content", out var contentProp))
                    {
                        vocabDict[idProp.GetInt32()] = contentProp.GetString() ?? string.Empty;
                    }
                }
                else if (item.ValueKind == JsonValueKind.Array)
                {
                    // Unigram format: ["token", score] - index is the token ID
                    var arr = item.EnumerateArray().ToArray();
                    if (arr.Length >= 1 && arr[0].ValueKind == JsonValueKind.String)
                    {
                        vocabDict[index] = arr[0].GetString() ?? string.Empty;
                    }
                }
                index++;
            }
        }
        else
        {
            throw new InvalidOperationException(
                $"Invalid tokenizer.json: 'model.vocab' has unexpected type '{vocab.ValueKind}'. " +
                "Expected Object (token â†’ id) or Array ([{{id, content}}]).");
        }

        if (vocabDict.Count == 0)
        {
            throw new InvalidOperationException("Invalid tokenizer.json: 'model.vocab' is empty");
        }

        // Create vocab.txt content
        var vocabLines = new StringBuilder();
        for (var i = 0; i < vocabDict.Count; i++)
        {
            vocabLines.AppendLine(vocabDict.TryGetValue(i, out var token) ? token : $"[unused{i}]");
        }

        var vocabBytes = Encoding.UTF8.GetBytes(vocabLines.ToString());
        using var vocabStream = new MemoryStream(vocabBytes);
        return WordPieceTokenizer.Create(vocabStream);
    }

    private static string? FindSentencePieceModel(string modelDir)
    {
        var patterns = new[]
        {
            "sentencepiece.bpe.model",
            "source.spm",
            "target.spm",
            "*.spm",
            "*.model"
        };

        foreach (var pattern in patterns)
        {
            var files = Directory.GetFiles(modelDir, pattern);
            if (files.Length > 0)
            {
                // Verify it's actually a SentencePiece model
                var file = files[0];
                if (IsSentencePieceModel(file))
                    return file;
            }
        }

        return null;
    }

    private static bool IsSentencePieceModel(string path)
    {
        try
        {
            using var stream = File.OpenRead(path);
            // Try to create a tokenizer - if it works, it's valid
            _ = LlamaTokenizer.Create(stream);
            return true;
        }
        catch
        {
            return false;
        }
    }

    private static Tokenizer? CreateBpeTokenizer(string modelDir)
    {
        var vocabPath = Path.Combine(modelDir, "vocab.json");
        var mergesPath = Path.Combine(modelDir, "merges.txt");

        if (File.Exists(vocabPath) && File.Exists(mergesPath))
        {
            using var vocabStream = File.OpenRead(vocabPath);
            using var mergesStream = File.OpenRead(mergesPath);
            return CodeGenTokenizer.Create(vocabStream, mergesStream);
        }

        return null;
    }

    private static Dictionary<string, int> LoadVocabularySync(string modelDir)
    {
        var vocabJsonPath = Path.Combine(modelDir, "vocab.json");
        if (File.Exists(vocabJsonPath))
        {
            var json = File.ReadAllText(vocabJsonPath);
            var vocab = new Dictionary<string, int>(StringComparer.Ordinal);

            try
            {
                using var doc = JsonDocument.Parse(json);
                ParseVocabElement(doc.RootElement, vocab);
            }
            catch
            {
                // Return empty vocab on parse failure
            }

            return vocab;
        }

        var tokenizerJsonPath = Path.Combine(modelDir, "tokenizer.json");
        if (File.Exists(tokenizerJsonPath))
        {
            var json = File.ReadAllText(tokenizerJsonPath);
            try
            {
                using var doc = JsonDocument.Parse(json);
                if (doc.RootElement.TryGetProperty("model", out var model) &&
                    model.TryGetProperty("vocab", out var vocabElement))
                {
                    var vocab = new Dictionary<string, int>(StringComparer.Ordinal);
                    ParseVocabElement(vocabElement, vocab);
                    return vocab;
                }
            }
            catch
            {
                // Fall through
            }
        }

        return [];
    }

    /// <summary>
    /// Parses vocab element handling both Object and Array formats.
    /// </summary>
    private static void ParseVocabElement(JsonElement element, Dictionary<string, int> vocab)
    {
        if (element.ValueKind == JsonValueKind.Object)
        {
            foreach (var property in element.EnumerateObject())
            {
                if (property.Value.TryGetInt32(out var id))
                {
                    vocab[property.Name] = id;
                }
            }
        }
        else if (element.ValueKind == JsonValueKind.Array)
        {
            // Handle array formats:
            // 1. [{"id": 0, "content": "[PAD]"}, ...] - Object items
            // 2. [["token", score], ...] - Unigram format (tuple-like arrays)
            var index = 0;
            foreach (var item in element.EnumerateArray())
            {
                if (item.ValueKind == JsonValueKind.Object)
                {
                    // Object format: {"id": 0, "content": "[PAD]"}
                    if (item.TryGetProperty("id", out var idProp) &&
                        item.TryGetProperty("content", out var contentProp) &&
                        idProp.TryGetInt32(out var id))
                    {
                        var content = contentProp.GetString();
                        if (content != null)
                        {
                            vocab[content] = id;
                        }
                    }
                }
                else if (item.ValueKind == JsonValueKind.Array)
                {
                    // Unigram format: ["token", score] - index is the token ID
                    var arr = item.EnumerateArray().ToArray();
                    if (arr.Length >= 1 && arr[0].ValueKind == JsonValueKind.String)
                    {
                        var token = arr[0].GetString();
                        if (token != null)
                        {
                            vocab[token] = index;
                        }
                    }
                }
                index++;
            }
        }
    }
}
